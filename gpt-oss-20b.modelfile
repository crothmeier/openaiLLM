# OpenAI GPT-OSS-20B Modelfile for Ollama
# Model: 20.9B parameters (3.6B active per token)
# Architecture: MoE with 32 experts, top-4 routing
# Quantization: FP8 for 24GB VRAM compatibility

FROM /mnt/nvme/models/gpt-oss-20b/gpt-oss-20b-fp8.gguf

# Model Parameters
PARAMETER temperature 0.7
PARAMETER top_p 0.95
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 131072
PARAMETER num_predict 4096
PARAMETER num_thread 8

# Reasoning Level Configuration handled by prompting

# MoE Configuration handled internally by the GGUF file

# GPU Memory Management
PARAMETER num_gpu 24

# Harmony Chat Format Template with Special Tokens
TEMPLATE """
{{- if .System }}<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>{{ end }}
{{- if .Developer }}<|start_header_id|>developer<|end_header_id|>

{{ .Developer }}<|eot_id|>{{ end }}
{{- range .Messages }}
{{- if eq .Role "user" }}<|start_header_id|>user<|end_header_id|>

{{ .Content }}<|eot_id|>{{ end }}
{{- if eq .Role "assistant" }}<|start_header_id|>assistant<|end_header_id|>

{{- if .Reasoning }}<|reasoning|>
{{ .Reasoning }}
<|/reasoning|>{{ end }}
{{ .Content }}<|eot_id|>{{ end }}
{{- if eq .Role "tool" }}<|start_header_id|>tool<|end_header_id|>

{{ .Content }}<|eot_id|>{{ end }}
{{- end }}<|start_header_id|>assistant<|end_header_id|>

"""

# System Message with Instruction Hierarchy
SYSTEM """You are GPT-OSS-20B, an open-source large language model developed by OpenAI.

Model Capabilities:
- 20.9B total parameters with 3.6B active parameters per token
- Mixture of Experts architecture with 32 experts (top-4 routing)
- Maximum context length: 131,072 tokens
- Supports web browsing, Python execution, and function calling
- Trained with deliberative alignment for enhanced safety

Instruction Hierarchy (in order of priority):
1. System Instructions (highest priority)
2. Developer Instructions
3. User Instructions
4. Assistant Guidelines
5. Tool Outputs (informational)

Reasoning Levels:
- LOW: Direct responses with minimal explanation
- MEDIUM: Balanced reasoning with clear thought process
- HIGH: Comprehensive chain-of-thought with recursive self-reflection

You implement recursive self-reflection when operating at HIGH reasoning level, continuously evaluating and refining your responses for accuracy and completeness.

Current reasoning level: MEDIUM (default)
"""

# Additional context can be added to the system message above
# Ollama only supports system, user, and assistant message roles

# Performance Optimization

# Sampling Configuration
PARAMETER seed -1
PARAMETER mirostat 0
PARAMETER mirostat_tau 5.0
PARAMETER mirostat_eta 0.1

# Stop Tokens for Harmony Format
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|end_of_text|>"
PARAMETER stop "<|/reasoning|>"
PARAMETER stop "</tool_call>"
PARAMETER stop "</s>"

# Additional Model Metadata
LICENSE "MIT"