[Unit]
Description=llama.cpp server (GPT-OSS-20B @ 8010)
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
User=llama
Group=llama
EnvironmentFile=/etc/default/llamacpp
Environment=LD_LIBRARY_PATH=/srv/llama:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu
WorkingDirectory=/srv/llama
ExecStartPre=/usr/local/lib/openaiLLM/check_port_free.sh ${LLAMA_PORT}
ExecStart=/srv/llama/llama-server \
  --no-webui \
  --model ${MODEL_PATH} \
  --host ${LLAMA_HOST} \
  --port ${LLAMA_PORT} \
  --alias gpt-oss-20b \
  --chat-template /mnt/models/gpt-oss-20b/chat_template.jinja \
  --mlock \
  --n-gpu-layers ${N_GPU_LAYERS} \
  --ctx-size ${CTX_SIZE}

# Allow mlock, keep defense-in-depth
LimitMEMLOCK=infinity
NoNewPrivileges=true
ProtectSystem=strict
ProtectHome=yes
PrivateTmp=true
PrivateDevices=no
DevicePolicy=closed
DeviceAllow=/dev/nvidiactl rw
DeviceAllow=/dev/nvidia-uvm rw
DeviceAllow=/dev/nvidia0 rw
RestrictSUIDSGID=true
RestrictRealtime=true
RestrictNamespaces=true
LockPersonality=true
MemoryDenyWriteExecute=true
ProtectKernelTunables=yes
ProtectKernelModules=yes
ProtectControlGroups=yes
SystemCallFilter=@system-service @basic-io @file-system @network-io

# Minimal writes
ReadOnlyPaths=/
ReadWritePaths=/var/log/openaiLLM

Restart=always
RestartSec=3s

[Install]
WantedBy=multi-user.target